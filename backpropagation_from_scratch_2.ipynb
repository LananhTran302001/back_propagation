{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load data and preprocess**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_iris()\n",
    "x = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "print(\"shape of input:\", x.shape)\n",
    "print(\"shape of output:\", y.shape)\n",
    "\n",
    "print(\"EXAMPLE:\")\n",
    "print(\"Input:\", x[0])\n",
    "print(\"Output:\", y[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let encode output with one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_with_one_hot_encoding(categorical_values):\n",
    "    num_of_classes = len(np.unique(categorical_values))\n",
    "    return np.eye(num_of_classes)[categorical_values]\n",
    "\n",
    "encoded_y = encode_with_one_hot_encoding(y)\n",
    "print(\"examples of encoded y:\")\n",
    "print(encoded_y[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transpose data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x.T\n",
    "Y = encoded_y.T\n",
    "\n",
    "print(\"input shape:\", X.shape)\n",
    "print(\"target shape:\", Y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_data(data):\n",
    "    return f'number of features is {data.shape[0]} and number of datapoints is {data.shape[1]}'\n",
    "\n",
    "print(f'For input: {describe_data(X)}')\n",
    "print(f'For output: {describe_data(Y)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating neural network**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In neural network, the total input $x_j$ to neuron j is a linear function of the output\n",
    "\n",
    "**Weights of layer l:**\n",
    "\n",
    "$\\theta^{(l)} = \\begin{bmatrix} \n",
    "                \\theta_{11}^{(l)} & \\theta_{12}^{(l)} & ... & \\theta_{1n}^{(l)} \\\\\n",
    "                \\theta_{21}^{(l)} & \\theta_{22}^{(l)} & ... & \\theta_{2n}^{(l)} \\\\\n",
    "                ... & ... & ... & ... \\\\\n",
    "                \\theta_{m1}^{(l)} & \\theta_{m2}^{(l)} & ... & \\theta_{mn}^{(l)} \\\\\n",
    "                \\end{bmatrix}\n",
    "$\n",
    "\n",
    "Where:\n",
    " - m is the number of input features\n",
    " - n is the number of neurons\n",
    " - $\\theta_{16}^{(2)}$ is the weight of the connection between the first feature and $6^{th}$ neuron in second layer\n",
    " - Shape of layer l 's weights is $m \\times n = $ number of features $\\times$ number of neurons"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create neural network and implement forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neural_network(input_size, output_size, hidden_sizes):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - input_size: \n",
    "        - output_size: number of neurons of the last layer\n",
    "        - hidden_sizes: number of neurons of hidden layers\n",
    "    \"\"\"\n",
    "    network_wetghts = []\n",
    "    layer_sizes = hidden_sizes\n",
    "    layer_sizes.append(output_size)\n",
    "    for num_of_neurons in layer_sizes:\n",
    "        layer_weights = np.random.rand(input_size, num_of_neurons)*2-1  # weights are initialized in range [-1, 1]\n",
    "        input_size = num_of_neurons\n",
    "        network_wetghts.append(layer_weights)\n",
    "    return network_wetghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_layer(layer_weights):\n",
    "    return f'there is {layer_weights.shape[1]} neurons with {layer_weights.shape[0]} inputs each'\n",
    "\n",
    "neural_network_weights = create_neural_network(X.shape[0], Y.shape[0], [8, 6])\n",
    "for idx, layer_weights in enumerate(neural_network_weights):\n",
    "    print(f'In layer {idx} {describe_layer(layer_weights)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a neural network as below image:\n",
    "\n",
    "Output of layer 1 will be calculated by:\n",
    "\n",
    "$A^{(1)} = (W^{(1)})^T \\times X^{T} $\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"network_wo_bias.png\" alt=\"simple neural network\" width=\"650\"/>\n",
    "</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But wait** ! Do you think we have forgot something ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! **Bias**\n",
    "\n",
    "Our neural network should be represented as:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"network.png\" alt=\"neural network\" width=\"650\"/>\n",
    "</p>\n",
    "\n",
    "Bias is an adjustable, numerical term added to a perceptronâ€™s weighted sum of inputs and weights. Bias allows perceptrons to account for situations where the inputs are insufficient to capture the complexity of the problem. It enables the perceptron to learn better decision boundaries by shifting them in the desired direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neural_network_with_bias(input_size, output_size, hidden_sizes):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - input_size: \n",
    "        - output_size: number of neurons of the last layer\n",
    "        - hidden_sizes: number of neurons of hidden layers\n",
    "    \"\"\"\n",
    "    network_weights = []\n",
    "    network_biases = []\n",
    "    layer_sizes = hidden_sizes\n",
    "    layer_sizes.append(output_size)\n",
    "    for num_of_neurons in layer_sizes:\n",
    "        layer_weights = np.random.rand(input_size, num_of_neurons)*2-1  # weights are initialized in range [-1, 1]\n",
    "        layer_bias = np.random.rand(1, num_of_neurons)*2-1  # bias are initialized in range [-1, 1]\n",
    "        network_weights.append(layer_weights)\n",
    "        network_biases.append(layer_bias)\n",
    "        input_size = num_of_neurons\n",
    "    return network_weights, network_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_bias(layer_bias):\n",
    "    return f'bias has shape {layer_bias.shape} for {layer_bias.shape[1]} neurons'\n",
    "\n",
    "neural_network_weights, neural_network_biases = create_neural_network_with_bias(X.shape[0], Y.shape[0], [8, 6])\n",
    "for idx, layer_weights in enumerate(neural_network_weights):\n",
    "    print(f'In layer {idx} {describe_layer(layer_weights)} and {describe_bias(neural_network_biases[idx])}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And **activation function**\n",
    "\n",
    "Activation function is used to determine the output of neural network like yes or no. It maps the resulting values in between 0 to 1 or -1 to 1 etc. (depending upon the function).\n",
    "\n",
    "In this example, we are classifying iris flowers. Therefore, sigmoid activation function is used for models to predict the probability because its output is always between 0 and 1. \n",
    "\n",
    "Sigmoid activation function:\n",
    "\n",
    "$\\sigma(u) = \\frac{1}{1 + e^{-u}}$\n",
    "\n",
    "Sigmoid derivative is:\n",
    "\n",
    "$\\frac{d}{du} \\sigma(u) = \\frac{d}{du} \\frac{1}{1 + e^{-u}} = -(1 + e^{-u})^{-2} \\cdot \\frac{d}{du} (1 + e^{-u})\n",
    "                        = -(1 + e^{-u})^{-2} \\cdot \\frac{d}{du} (e^{-u})\n",
    "                        = -(1 + e^{-u})^{-2} \\cdot e^{-u} \\cdot \\frac{d}{du} (-u)\n",
    "                        = (1 + e^{-u})^{-2} \\cdot e^{-u} \n",
    "                        = \\sigma(u) \\cdot (1 - \\sigma(u))\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(u):\n",
    "    return 1 / (1 + np.exp(-u))\n",
    "\n",
    "def sigmoid_derivative(u):\n",
    "    a = sigmoid(u)\n",
    "    return a * (1.0 - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to understand sigmoid and its derivative\n",
    "demo_x = np.linspace(-10, 10, 100, endpoint=True)\n",
    "demo_y = sigmoid(demo_x)\n",
    "demo_derivative = sigmoid_derivative(demo_x)\n",
    "plt.plot(demo_x, demo_y, label='Sigmoid function')\n",
    "plt.plot(demo_x, demo_derivative, label='Sigmoid derivative')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Forward propagation**\n",
    "\n",
    "Output of layer 1 will be calculated by:\n",
    "\n",
    "$A^{(1)} = (W^{(1)})^T \\times X^{T} + (b^{(1)})^{T}$\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"feed_forward_1.png\" alt=\"feed forward\" width=\"650\"/>\n",
    "</p>\n",
    "\n",
    "Let's do a simplifying trick:\n",
    "\n",
    "Extend input by adding 1 to every input vector. Bias becomes the weight on this extra input. It can be treated just like other weights. This trick is used to be able to include bias in weights\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"feed_forward_2.png\" alt=\"simplify feed forward\" width=\"650\"/>\n",
    "</p>\n",
    "\n",
    "Output of layer 1 will be calculated by:\n",
    "\n",
    "$A^{(1)} = \\sigma ((W^{(1)})^T \\times X^{T} + (b^{(1)})^{T}) = \\sigma ((W^{(1)} || b^{(1)})^T \\times (X || 1)^{T})$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend input with 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_input_with_ones(input_T):\n",
    "    return np.concatenate((input_T, np.ones(shape=(1, input_T.shape[1]))), axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend weights with bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_weights_with_bias(weights, bias):\n",
    "    return np.concatenate((weights, bias), axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed forward code for neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(input_data, network_weights, network_biases):\n",
    "    layer_input = extend_input_with_ones(input_data)\n",
    "    layers_output = []\n",
    "    for layer_weights, layer_bias in zip(network_weights, network_biases):     # same meaning as for layer in network\n",
    "        extended_weights = extend_weights_with_bias(layer_weights, layer_bias)  # extended weights include bias\n",
    "        layer_output = sigmoid(extended_weights.T @ layer_input) # @ denotes matrix multiply (different from * which denotes dot multiply)\n",
    "        layer_input = extend_input_with_ones(layer_output) # input of next layer = output of current layer extend with ones\n",
    "        layers_output.append(layer_output)\n",
    "    return layers_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function**\n",
    "\n",
    "In this example, we use mean square error as loss function\n",
    "\n",
    "$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (gt_{i} - pred_{i})^2 \n",
    "= \\frac{1}{n} \\sum (Y - \\hat{Y})^2 \n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse(groundtruth, prediction):\n",
    "    return np.sum((groundtruth - prediction)**2)/len(groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed forward\n",
    "outputs = feed_forward(X, neural_network_weights, neural_network_biases)\n",
    "prediction = outputs[-1]\n",
    "\n",
    "# Describe output of each layer\n",
    "for idx, layer_output in enumerate(outputs):\n",
    "    print(f'Layer {idx} output has {describe_data(layer_output)}')\n",
    "\n",
    "# Calculate MSE\n",
    "mse  = calculate_mse(Y, prediction)\n",
    "print(f'MSE = {mse}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take this step slower to understand it.**\n",
    "\n",
    "![Neural network](network.png)\n",
    "\n",
    "First, you have input data X has 4 features $\\times$ 150 samples.\n",
    "\n",
    "And a neural network has 3 layers:\n",
    " - Layer 1 has 8 neurons\n",
    " - Layer 2 has 6 neurons\n",
    " - Layer 3 has 3 neurons\n",
    "\n",
    "As we mentioned above, weights matrix of each layer has shape = number of input features $\\times$ number of neurons in that layer. \n",
    "\n",
    "Therefore:\n",
    " - Layer 1: weights matrix has shape 4 $\\times$ 8, bias matrix has shape 1 $\\times$ 8\n",
    " - Layer 2 weights matrix has shape 8 $\\times$ 6, bias matrix has shape 1 $\\times$ 6\n",
    " - Layer 3 weights matrix has shape 6 $\\times$ 3, bias matrix has shape 1 $\\times$ 3\n",
    "\n",
    "Output of each layer is: $\\text{weights}^{T} \\times \\text{layer input}^{T} + \\text{bias}^{T}$. Then:\n",
    " - Output of layer 1 has shape of (8, 4) $\\times$ (4, 150) + broadcast(8, 1)= (8, 150)\n",
    " - Output of layer 2 has shape of (6, 8) $\\times$ (8, 150)  + broadcast(6, 1)= (6, 150)\n",
    " - Output of layer 3 has shape of (3, 6) $\\times$ (6, 150) + broadcast(3, 1)= (3, 150). This is shape of the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode the output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(prediction, axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Backward propagation**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, calculate the gradient of loss function MSE with respect to weights.\n",
    "\n",
    "![computation graph and MSE loss](MSE_loss.png)\n",
    "\n",
    "As $MSE =  \\frac{1}{n} \\sum (Y - \\hat{Y})^2 $\n",
    "\n",
    "Let's consider: $L = (Y - \\hat{Y})^2$\n",
    "\n",
    "Then:\n",
    "$\\frac{dL}{d\\hat{Y}} = d\\frac{(Y - \\hat{Y})^2}{d\\hat{Y}} = 2(Y - \\hat{Y})d \\frac{Y - \\hat{Y}}{d\\hat{Y}} = -2(Y - \\hat{Y}) = 2(\\hat{Y}-Y)$\n",
    "\n",
    "But: $\\hat{Y} = \\sigma(W^{(3)} \\times A^{(2)} + b^{(3)})$\n",
    "then: \n",
    "$\n",
    "\\frac{d\\hat{Y}}{dW^{(3)}} = d\\frac{\\sigma(W^{(3)} \\times A^{(2)} + b^{(3)})}{dW^{(3)}} \n",
    "$\n",
    "\n",
    "Gradient of L respect to **weights**:\n",
    "\n",
    "$\\frac{dL}{dW^{(3)}} = \\frac{dL}{d\\hat{Y}}.\\frac{d\\hat{Y}}{dW^{(3)}} \n",
    "= 2(\\hat{Y}-Y) d\\frac{\\sigma(W^{(3)} \\times A^{(2)} + b^{(3)})}{dW^{(3)}} \n",
    "= 2(\\hat{Y}-Y) A^{(2)} d\\frac{\\sigma(W^{(3)} \\times A^{(2)} + b^{(3)})}{d(W^{(3)} \\times A^{(2)} + b^{(3)})}\n",
    "$\n",
    "\n",
    "Additionally:\n",
    "\n",
    "$\\frac{dL}{dW^{(2)}} = \\frac{dL}{d\\hat{Y}}.\\frac{d\\hat{Y}}{dA^{(2)}}.\\frac{dA^{(2)}}{dW^{(2)}}\n",
    "= 2(\\hat{Y}-Y) \n",
    "d\\frac{\\sigma(W^{(3)} \\times A^{(2)} + b^{(3)})}{dA^{(2)}} \n",
    "d\\frac{\\sigma(W^{(2)} \\times A^{(1)} + b^{(2)})}{dW^{(2)}} \n",
    "= 2(\\hat{Y}-Y) W^{(3)} \n",
    "d\\frac{\\sigma(W^{(3)} \\times A^{(2)} + b^{(3)})}{d(W^{(3)} \\times A^{(2)} + b^{(3)})} A^{(1)} \n",
    "d\\frac{\\sigma(W^{(2)} \\times A^{(1)} + b^{(2)})}{d(W^{(2)} \\times A^{(1)} + b^{(2)})} \n",
    "$\n",
    "\n",
    "$\\frac{dL}{dW^{(1)}} = \\frac{dL}{d\\hat{Y}}.\\frac{d\\hat{Y}}{dA^{(2)}}.\\frac{dA^{(2)}}{dA^{(1)}}.\\frac{dA^{(1)}}{dW^{(1)}}\n",
    "= 2(\\hat{Y}-Y) d\\frac{\\sigma(W^{(3)} \\times A^{(2)} + b^{(3)})}{dA^{(2)}} \n",
    "d\\frac{\\sigma(W^{(2)} \\times A^{(1)} + b^{(2)})}{dA^{(1)}} \n",
    "d\\frac{\\sigma(W^{(1)} \\times X + b^{(1)})}{dW^{(1)}} \n",
    "= 2(\\hat{Y}-Y) W^{(3)} \n",
    "d\\frac{\\sigma(W^{(3)} \\times A^{(2)} + b^{(3)})}{d(W^{(3)} \\times A^{(2)} + b^{(3)})} W^{(2)} \n",
    "d\\frac{\\sigma(W^{(2)} \\times A^{(1)} + b^{(2)})}{d(W^{(2)} \\times A^{(1)} + b^{(2)})} X \n",
    "d\\frac{\\sigma(W^{(1)} \\times X + b^{(1)})}{d(W^{(1)} \\times X + b^{(1)})} \n",
    "$\n",
    "\n",
    "\n",
    "Gradient of L respect to **bias**:\n",
    "\n",
    "$\\frac{dL}{db^{(3)}} = \\frac{dL}{d\\hat{Y}}.\\frac{d\\hat{Y}}{db^{(3)}} \n",
    "= 2(\\hat{Y}-Y) d\\frac{\\sigma(W^{(3)} \\times A^{(2)} + b^{(3)})}{db^{(3)}} \n",
    "= 2(\\hat{Y}-Y) d\\frac{\\sigma(W^{(3)} \\times A^{(2)} + b^{(3)})}{d(W^{(3)} \\times A^{(2)} + b^{(3)})}\n",
    "$\n",
    "\n",
    "Additionally:\n",
    "\n",
    "$\\frac{dL}{db^{(2)}} = \\frac{dL}{d\\hat{Y}}.\\frac{d\\hat{Y}}{dA^{(2)}}.\\frac{dA^{(2)}}{db^{(2)}}\n",
    "= 2(\\hat{Y}-Y) \n",
    "d\\frac{\\sigma(W^{(3)} \\times A^{(2)} + b^{(3)})}{dA^{(2)}} \n",
    "d\\frac{\\sigma(W^{(2)} \\times A^{(1)} + b^{(2)})}{db^{(2)}} \n",
    "= 2(\\hat{Y}-Y) W^{(3)} \n",
    "d\\frac{\\sigma(W^{(3)} \\times A^{(2)} + b^{(3)})}{d(W^{(3)} \\times A^{(2)} + b^{(3)})} \n",
    "d\\frac{\\sigma(W^{(2)} \\times A^{(1)} + b^{(2)})}{d(W^{(2)} \\times A^{(1)} + b^{(2)})} \n",
    "$\n",
    "\n",
    "$\\frac{dL}{db^{(1)}} = \\frac{dL}{d\\hat{Y}}.\\frac{d\\hat{Y}}{dA^{(2)}}.\\frac{dA^{(2)}}{dA^{(1)}}.\\frac{dA^{(1)}}{dW^{(1)}}\n",
    "= 2(\\hat{Y}-Y) d\\frac{\\sigma(W^{(3)} \\times A^{(2)} + b^{(3)})}{dA^{(2)}} \n",
    "d\\frac{\\sigma(W^{(2)} \\times A^{(1)} + b^{(2)})}{dA^{(1)}} \n",
    "d\\frac{\\sigma(W^{(1)} \\times X + b^{(1)})}{dW^{(1)}} \n",
    "= 2(\\hat{Y}-Y) W^{(3)} \n",
    "d\\frac{\\sigma(W^{(3)} \\times A^{(2)} + b^{(3)})}{d(W^{(3)} \\times A^{(2)} + b^{(3)})} W^{(2)} \n",
    "d\\frac{\\sigma(W^{(2)} \\times A^{(1)} + b^{(2)})}{d(W^{(2)} \\times A^{(1)} + b^{(2)})} \n",
    "d\\frac{\\sigma(W^{(1)} \\times X + b^{(1)})}{d(W^{(1)} \\times X + b^{(1)})} \n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(network_weights, network_biases, input, outputs, target):\n",
    "    ''' \n",
    "    Parameters:\n",
    "        - network: [W1, W2, W3]\n",
    "        - outputs: [a1, a2, y_hat]\n",
    "        - target: y\n",
    "    Return:\n",
    "        - Gradient of loss function respect to model weights & model biases\n",
    "    '''\n",
    "\n",
    "    gradients_weights = []\n",
    "    gradients_biases = []\n",
    "\n",
    "    # Create A = [X, a1, a2, y_hat]\n",
    "    A = [input]\n",
    "    for output in outputs:\n",
    "        A.append(output)\n",
    "    \n",
    "    # delta = 2 * (y_hat - y) * sigmoid_derivative(y_hat)\n",
    "    delta = 2 * (outputs[-1] - target) * sigmoid_derivative(A[-1])\n",
    "    \n",
    "    # loop over the layers in the network in reverse order\n",
    "    for layer_idx in reversed(range(0, len(network_weights)-1)):\n",
    "        print(f'back propagate in layer {layer_idx}')\n",
    "        gradients_weights.append(np.dot(A[layer_idx].T, delta))    \n",
    "        gradients_biases.append((np.sum(delta, 0)).reshape(-1,1))\n",
    "        delta = (delta * sigmoid_derivative(A[layer_idx])).dot(gradients_weights[layer_idx].T)    \n",
    "\n",
    "    return zip(reversed(gradients_weights), reversed(gradients_biases))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients_weights, gradients_biases = backpropagate(neural_network_weights, neural_network_biases, X, outputs, Y)\n",
    "for idx, gw in enumerate(gradients_weights):\n",
    "    print(f'Gradient weight of {idx} layer: {describe_data(gw)}')\n",
    "for idx, gb in enumerate(gradients_biases):\n",
    "    print(f'Gradient weight of {idx} layer: {describe_data(gb)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then update weights and biases by gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(network_weights, network_biases, gradients_weights, gradients_biases , lr):\n",
    "    for i in range(len(network_weights)):\n",
    "        network_weights[i] = network_weights[i] - lr * gradients_weights[i]\n",
    "        network_biases[i] = network_biases[i] - lr * gradients_biases[i]\n",
    "    return network_weights, network_biases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then adjust weights by weights changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_weights(network, changes):\n",
    "    new_network = []\n",
    "    for weights, change in zip(network, changes):\n",
    "        new_weights = weights - change\n",
    "        new_network.append(new_weights)\n",
    "    return new_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = adjust_weights(neural_network, changes)\n",
    "\n",
    "# Describe each layer\n",
    "for idx, layer in enumerate(neural_network):\n",
    "    print(f'In layer {idx} {describe_layer(layer)}')\n",
    "\n",
    "# Feed forward\n",
    "prediction = feed_forward(X, neural_network)[-1]\n",
    "\n",
    "# Calculate MSE\n",
    "mse  = calculate_mse(Y, prediction)\n",
    "print(f'MSE = {mse}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulation! Here you come"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, input, target, lr, epochs):\n",
    "    mse_history = []\n",
    "    for epoch in epochs:\n",
    "        outputs = feed_forward(input, network)\n",
    "        mse_history.append(calculate_mse(outputs[-1], target))\n",
    "        gradients = backpropagate(network, outputs, target)\n",
    "        changes = calculate_weights_changes(network, input, outputs, gradients, lr)\n",
    "        network = adjust_weights(network, changes)\n",
    "    mse_history.append(calculate_mse(target, outputs[-1]))\n",
    "    return network, mse_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "\n",
    "1. [https://nttuan8.com/bai-4-backpropagation/](https://nttuan8.com/bai-4-backpropagation/)\n",
    "2. [https://youtu.be/Cwr0H0kAMtM?si=eG_B1AgeywbuZu0N](https://youtu.be/Cwr0H0kAMtM?si=eG_B1AgeywbuZu0N)\n",
    "3. [http://cs231n.stanford.edu/slides/2023/lecture_2.pdf](http://cs231n.stanford.edu/slides/2023/lecture_2.pdf)\n",
    "4. [http://cs231n.stanford.edu/schedule.html](http://cs231n.stanford.edu/schedule.html)\n",
    "5. [http://cs231n.stanford.edu/handouts/linear-backprop.pdf](http://cs231n.stanford.edu/handouts/linear-backprop.pdf)\n",
    "6. [https://pyimagesearch.com/2021/05/06/backpropagation-from-scratch-with-python/](https://pyimagesearch.com/2021/05/06/backpropagation-from-scratch-with-python/)\n",
    "7. [http://neuralnetworksanddeeplearning.com/chap2.html](http://neuralnetworksanddeeplearning.com/chap2.html)\n",
    "8. [https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/](https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/)\n",
    "9. [https://dustinstansbury.github.io/theclevermachine/a-gentle-introduction-to-neural-networks](https://dustinstansbury.github.io/theclevermachine/a-gentle-introduction-to-neural-networks)\n",
    "10. [https://dustinstansbury.github.io/theclevermachine/derivation-backpropagation](https://dustinstansbury.github.io/theclevermachine/derivation-backpropagation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
